main:
  dev_mode:
    debugging: 
    toy_test: 

  wandb:
    project_name: ismb2024_molpla_experiments_final_new_world
    session_name: rpower0_base_setting
    group_name: 

  ddp:
    port: 8700

  path:
    dataset: /ssd0/mogan/MolPLA/datasets
    checkpoint: /ssd0/mogan/MolPLA/checkpoints

  dataprep:
    dataset: geom
    version: v27
    subsample: 1.0
    filter_rare_arms: False    # don't touch
    filter_partial_mols: False # don't touch

  experiment:
    testing_mode: false
    random_seed: 911012
    which_best: loss

  model_params:
    model_type: molpla # FIXED
    hidden_dim: 300
    dropout_rate: 0.0
    graph_encoder: GNN
    gnn_params:
      aggr: add
      JK: last
      gnn_type: gin
      num_layer: 5
      norm_type: layernorm

    graph_pooling:   avg
    graph_projector: mlp
    node_projector:  mlp 
    link_decoder:    mlp

    stop_gradient_P: False
    stop_gradient_R: False
    stop_gradient_Q: True
    prop_conditioned: rgroups

    faiss_metric: inner_product

  train_params:
    batch_size: 2048
    num_epochs: 100
    num_warmups: 10

    optimizer: Adam
    scheduler: CosineAnnealingLR

    learning_rate: 0.0001
    weight_decay: 0.0

    early_stopping: loss
    early_patience: 30

    pretraining:
      graph_contrastive:
        score_func: dualentropy
        tau: 0.1

      linker_contrastive:
        score_func: dualentropy
        tau: 0.05
        negation: False

      rgroup_contrastive:
        score_func: dualentropy
        tau: 0.01
        negation: False

main_bench:
  dataprep:
    dataset: 
    version: 
    subsample:

  experiment:
    testing_mode: false
    random_seed: 8888
    which_best: loss

  model_params:
    dropout_rate: 0.2

  train_params:
    batch_size: 512
    num_epochs: 200
    num_warmups: 10

    optimizer: AdamW
    scheduler: dummy

    learning_rate: 0.0001
    weight_decay:  0.01

    early_stopping: 
    early_patience: 30

    finetuning:
      from_pretrained:   pretrained_geom_v27
      from_scratch: False
      freeze_pretrained: False